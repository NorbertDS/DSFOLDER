{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df38d48",
   "metadata": {},
   "source": [
    "## Preprocessing Steps for Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of Data Science which deals with Text data. \n",
    "\n",
    "Apart from numerical data, Text data is available to a great extent which is used to analyze and solve business problems. But before using the data for analysis or prediction, processing the data is important.\n",
    "\n",
    "To prepare the text data for the model building we perform text preprocessing. It is the very first step of NLP projects\n",
    "\n",
    "Text preprocessing is an essential step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for analysis. It includes Removing punctuations, Removing URLs, Removing Stop words, Lower casing, Tokenization, Stemming, and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc804d",
   "metadata": {},
   "source": [
    "### SMS Spam Data\n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,572 messages, tagged acording being ham (legitimate) or spam.\n",
    "\n",
    "The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9255b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "# detect the type of encoding in csv to help read the csv correctly\n",
    "import chardet\n",
    "def detect_encoding(csv_file_path):\n",
    "    ''' This function reads a chunk of the file to detect the encoding\n",
    "    \n",
    "    rb: The mode parameter specifies the mode in which the file is opened. 'rb' stands for \n",
    "    \"read binary\" mode. \n",
    "    \n",
    "    used with non-text files or when you want to explicitly read file content as bytes.'''\n",
    "    \n",
    "    with open(csv_file_path, 'rb') as f:\n",
    "        #read the file\n",
    "        rawdata = f.read(1024)\n",
    "    # Use chardet to detect the encoding\n",
    "    result = chardet.detect(rawdata)\n",
    "    encoding = result['encoding']\n",
    "    # result = chardet.detect(rawdata)['encoding']\n",
    "    return encoding\n",
    "\n",
    "\n",
    "encoding_type = detect_encoding(r'C:\\Users\\ELITEBOOK 840 G3\\OneDrive\\Desktop\\REGEX and NLP\\spam.csv')\n",
    "print(encoding_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00cebda",
   "metadata": {},
   "source": [
    "Chardet is a library that detects the encoding of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9de30d",
   "metadata": {},
   "source": [
    "Specifying Encoding: Since CSV files can be encoded in different formats, it's important to specify the correct encoding when reading the file. If the encoding is not specified or incorrect, it may lead to misinterpretation of characters, resulting in data corruption or errors.\n",
    "\n",
    "By specifying the encoding=\"ISO-8859-1\" parameter, the code ensures that the CSV file \"spam.csv\" is read using the ISO-8859-1 character encoding. This ensures that the data is correctly interpreted and displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b50402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the dataset and view what the data looks like\n",
    "data = pd.read_csv(r'C:\\Users\\ELITEBOOK 840 G3\\OneDrive\\Desktop\\REGEX and NLP\\spam.csv', encoding ='ISO-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3f89a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0  ham    \n",
       "1  ham    \n",
       "2  spam   \n",
       "3  ham    \n",
       "4  ham    \n",
       "\n",
       "                                                                                                                                                            v2  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                              \n",
       "1  Ok lar... Joking wif u oni...                                                                                                                                \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3  U dun say so early hor... U c already then say...                                                                                                            \n",
       "4  Nah I don't think he goes to usf, he lives around here though                                                                                                "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set display option to show entire content of cells\n",
    "pd.set_option('display.max_colwidth', 1)\n",
    "df = data.iloc[:, :2]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe77d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d3a69",
   "metadata": {},
   "source": [
    "The data has 5572 rows and 2 columns.\n",
    "\n",
    "Let’s check the dependent variable distribution between spam and ham. What is the dependent variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8579b283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1\n",
       "ham     4825\n",
       "spam    747 \n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the count of the dependent variable\n",
    "df['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2675b8",
   "metadata": {},
   "source": [
    "### Steps to Clean the Data\n",
    "\n",
    "#### 1. Punctuation Removal\n",
    "In this step, all the punctuations from the text are removed. string library of Python contains some pre-defined list of punctuations such as ‘!”#$%&'()*+,-./:;?@[\\]^_`{|}~’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4dea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK 840 G3\\AppData\\Local\\Temp\\ipykernel_7572\\999963391.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_msg'] = df['v2'].apply(remove_punctuation_from_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0  ham    \n",
       "1  ham    \n",
       "2  spam   \n",
       "3  ham    \n",
       "4  ham    \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
       "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3  U dun say so early hor... U c already then say...                                                                                                             \n",
       "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
       "\n",
       "                                                                                                                                               clean_msg  \n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                 \n",
       "1  Ok lar Joking wif u oni                                                                                                                                \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s  \n",
       "3  U dun say so early hor U c already then say                                                                                                            \n",
       "4  Nah I dont think he goes to usf he lives around here though                                                                                            "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#library that contains punctuation and Regex \n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#defining the function to remove punctuation\n",
    "def remove_punctuation_from_text(text):\n",
    "    \"\"\" \n",
    "    Remove punctuation characters from a text string.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text string containing punctuation.\n",
    "\n",
    "    Returns:\n",
    "    str: Text string with all punctuation characters removed.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match punctuation characters\n",
    "    punctuation_pattern = '['+ re.escape(string.punctuation) +']'\n",
    "    # Use regex to substitute punctuation characters with an empty string\n",
    "    punctuation_free_text = re.sub(punctuation_pattern, '', text)\n",
    "    return punctuation_free_text\n",
    "\n",
    "#storing the puntuation free text\n",
    "df['clean_msg'] = df['v2'].apply(remove_punctuation_from_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e7986",
   "metadata": {},
   "source": [
    "string.punctuation is a string provided by Python that contains all punctuation characters.\n",
    "\n",
    "re.escape() is a function that escapes special characters in a string so they can be treated as literal characters in a regular expression.\n",
    "\n",
    "The pattern [...] specifies a character set in a regular expression, where the characters inside the square brackets are the characters to match.\n",
    "\n",
    "re_sub() is  used for replacing text in a string using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d486ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2 Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d85a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to remove punctuation\n",
    "#def remove_punctuation(text):\n",
    "    #\"\"\"\" \n",
    "    #This function takes a text string as input and removes all punctuation characters from it. \n",
    "    #It achieves this using list comprehension to iterate over each character in the input text and\n",
    "    #retaining only those characters that are not present in the string.punctuation string.\n",
    "    \n",
    "    #\"\"\"\n",
    "    #punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    #return punctuationfree\n",
    "\n",
    "#storing the puntuation free text\n",
    "#df['clean_msg']= df['v2'].apply(lambda x:remove_punctuation(x))\n",
    "#df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37cb87",
   "metadata": {},
   "source": [
    "It iterates over each character i in the text string and filters out characters that are not present in the string.punctuation string. Essentially, it creates a new list containing only the characters from text that are not punctuation characters.\n",
    "\n",
    "The join() method is then used to concatenate the characters from the filtered list into a single string. The empty string \"\" before the join() method specifies that we want to join the characters together without any separator between them.\n",
    "\n",
    "The function is applied to each element of the 'v2' column in a DataFrame (df) using the apply method. The result is stored in a new column named 'clean_msg'.\n",
    "\n",
    "The lambda function is used to apply remove_punctuation to each element of the 'v2' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51651cfc",
   "metadata": {},
   "source": [
    " All the punctuations are removed from v2 and stored in the clean_msg column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34954982",
   "metadata": {},
   "source": [
    "#### 2. Lowering the Text\n",
    "It is one of the most common text preprocessing Python steps where the text is converted into the same case preferably lower case. But it is not necessary to do this step every time you are working on an NLP problem as for some problems lower casing can lead to loss of information.\n",
    "\n",
    "For example, if in any project we are dealing with the emotions of a person, then the words written in upper cases can be a sign of frustration or excitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412eb93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELITEBOOK 840 G3\\AppData\\Local\\Temp\\ipykernel_7572\\335452073.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['msg_lower']= df['clean_msg'].apply(lambda x: x.lower())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>msg_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0  ham    \n",
       "1  ham    \n",
       "2  spam   \n",
       "3  ham    \n",
       "4  ham    \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
       "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3  U dun say so early hor... U c already then say...                                                                                                             \n",
       "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
       "\n",
       "                                                                                                                                               clean_msg  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
       "1  Ok lar Joking wif u oni                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3  U dun say so early hor U c already then say                                                                                                             \n",
       "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                               msg_lower  \n",
       "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                 \n",
       "1  ok lar joking wif u oni                                                                                                                                \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s  \n",
       "3  u dun say so early hor u c already then say                                                                                                            \n",
       "4  nah i dont think he goes to usf he lives around here though                                                                                            "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['msg_lower']= df['clean_msg'].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252be604",
   "metadata": {},
   "source": [
    "All the text of clean_msg column are converted into lower case and stored in msg_lower column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846ac6b",
   "metadata": {},
   "source": [
    "#### 3. Tokenization\n",
    "Tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens could be words, phrases, symbols, or other meaningful elements depending on the context of the text and the requirements of the task at hand. \n",
    "\n",
    "In this step, the text is split into smaller units. We can use either sentence tokenization (sent_tokenize) or word tokenization (word_tokenize) or regex based on our problem statement. In this case we use regex but you can also use word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b097ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>msg_lower</th>\n",
       "      <th>msg_tokenied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0  ham    \n",
       "1  ham    \n",
       "2  spam   \n",
       "3  ham    \n",
       "4  ham    \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
       "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3  U dun say so early hor... U c already then say...                                                                                                             \n",
       "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
       "\n",
       "                                                                                                                                               clean_msg  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
       "1  Ok lar Joking wif u oni                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3  U dun say so early hor U c already then say                                                                                                             \n",
       "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                               msg_lower  \\\n",
       "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                  \n",
       "1  ok lar joking wif u oni                                                                                                                                 \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3  u dun say so early hor u c already then say                                                                                                             \n",
       "4  nah i dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                                                         msg_tokenied  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                                         \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                                      \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "3  [u, dun, say, so, early, hor, u, c, already, then, say]                                                                                                                             \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]                                                                                                           "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining function for tokenization\n",
    "import re\n",
    "def tokenization(text):\n",
    "    ''' This function takes a text string as input and returns a list of tokens (words) \n",
    "    extracted from the text.'''\n",
    "    \n",
    "    tokens = re.split('[^\\w]+',text)\n",
    "    return tokens\n",
    "#applying function to the column\n",
    "df['msg_tokenied']= df['msg_lower'].apply(lambda x: tokenization(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d29f0e",
   "metadata": {},
   "source": [
    "This function, tokenization, takes a text string as input and returns a list of tokens (words) extracted from the text.\n",
    "\n",
    "The regular expression pattern '[^\\w]+' matches one or more non-word characters (i.e., characters that are not letters, digits, or underscores).\n",
    "\n",
    "The re.split() function splits the text wherever this pattern occurs, resulting in a list of tokens.\n",
    "\n",
    "**Try with word tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2d9ce",
   "metadata": {},
   "source": [
    "#### Stop Word Removal\n",
    "\n",
    "Stopwords are the commonly used words and are removed from the text as they do not add any value to the analysis. These words carry less or no meaning.\n",
    "\n",
    "NLTK library consists of a list of words that are considered stopwords for the English language. Some of them are : [i, me, my, myself, we, our, ours, ourselves, you, you’re, you’ve, you’ll, you’d, your, yours, yourself, yourselves, he, most, other, some, such, no, nor, not, only, own, same, so, then, too, very, s, t, can, will, just, don, don’t, should, should’ve, now, d, ll, m, o, re, ve, y, ain, aren’t, could, couldn’t, didn’t, didn’t]\n",
    "\n",
    "But it is not necessary to use the provided list as stopwords as they should be chosen wisely based on the project. For example, ‘How’ can be a stop word for a model but can be important for some other problem where we are working on the queries of the customers. We can create a customized list of stop words for different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd7c56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing nlp library\n",
    "import nltk\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2625b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>msg_lower</th>\n",
       "      <th>msg_tokenied</th>\n",
       "      <th>msg_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0  ham    \n",
       "1  ham    \n",
       "2  spam   \n",
       "3  ham    \n",
       "4  ham    \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
       "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3  U dun say so early hor... U c already then say...                                                                                                             \n",
       "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
       "\n",
       "                                                                                                                                               clean_msg  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
       "1  Ok lar Joking wif u oni                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3  U dun say so early hor U c already then say                                                                                                             \n",
       "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                               msg_lower  \\\n",
       "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                  \n",
       "1  ok lar joking wif u oni                                                                                                                                 \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3  u dun say so early hor u c already then say                                                                                                             \n",
       "4  nah i dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                                                         msg_tokenied  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                                          \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                                       \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, then, say]                                                                                                                              \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]                                                                                                            \n",
       "\n",
       "                                                                                                                                                  msg_no_stopwords  \n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                                              \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "3  [u, dun, say, early, hor, u, c, already, say]                                                                                                                    \n",
       "4  [nah, dont, think, goes, usf, lives, around, though]                                                                                                             "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove stopwords from tokenized text\n",
    "import re\n",
    "def remove_stopwords(tokens):\n",
    "    # Define a regex pattern to match stopwords\n",
    "    stopwords_pattern = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "    \n",
    "    # Remove stopwords from the list of tokens using regex\n",
    "    filtered_tokens = [token for token in tokens if not re.search(stopwords_pattern, token)]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply remove_stopwords function to 'msg_tokenized' column\n",
    "df['msg_no_stopwords'] = df['msg_tokenied'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab943b2",
   "metadata": {},
   "source": [
    "\\b is a word boundary anchor that matches the position between a word character and a non-word character.\n",
    "\n",
    "(?:{}) is a non-capturing group that contains the list of stopwords joined by the | (OR) operator.\n",
    "\n",
    "The format() method inserts the list of stopwords into the pattern.\n",
    "This pattern matches any word that is a stopword.\n",
    "\n",
    "_{} (Curley)_: Used to specify an exact number or range of \n",
    "repetitions\n",
    "\n",
    "Capturing groups are used to capture and remember the text matched by the pattern inside the parentheses.\n",
    "\n",
    "Non-capturing groups are defined using (?: ) syntax in a regular expression.\n",
    "They are used when you want to match a pattern but don't need to capture the matched text as a separate group.\n",
    "\n",
    "Non-capturing groups are especially useful when you want to use grouping for logical grouping or alternation (with |), but you don't want to create a capturing group.\n",
    "\n",
    "re.search() is used to check if each token does not match the stopwords pattern.It filters out tokens that match the stopwords pattern, leaving only tokens that do not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3248df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\\\b(?:i|me|my|myself|we|our|ours|ourselves|you|you're|you've|you'll|you'd|your|yours|yourself|yourselves|he|him|his|himself|she|she's|her|hers|herself|it|it's|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|that'll|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|don't|should|should've|now|d|ll|m|o|re|ve|y|ain|aren|aren't|couldn|couldn't|didn|didn't|doesn|doesn't|hadn|hadn't|hasn|hasn't|haven|haven't|isn|isn't|ma|mightn|mightn't|mustn|mustn't|needn|needn't|shan|shan't|shouldn|shouldn't|wasn|wasn't|weren|weren't|won|won't|wouldn|wouldn't)\\\\b\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_pattern = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "stopwords_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efbfe3",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "It is also known as the text standardization step where the words are stemmed or diminished to their root/base form.\n",
    "\n",
    "But the disadvantage of stemming is that it stems the words such that its root form loses the meaning or it is not diminished to a proper English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a8027c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ELITEBOOK 840\n",
      "[nltk_data]     G3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>msg_lower</th>\n",
       "      <th>msg_tokenied</th>\n",
       "      <th>msg_no_stopwords</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "      <th>msg_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0  ham    \n",
       "1  ham    \n",
       "2  spam   \n",
       "3  ham    \n",
       "4  ham    \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
       "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3  U dun say so early hor... U c already then say...                                                                                                             \n",
       "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
       "\n",
       "                                                                                                                                               clean_msg  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
       "1  Ok lar Joking wif u oni                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3  U dun say so early hor U c already then say                                                                                                             \n",
       "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                               msg_lower  \\\n",
       "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                  \n",
       "1  ok lar joking wif u oni                                                                                                                                 \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3  u dun say so early hor u c already then say                                                                                                             \n",
       "4  nah i dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                                                         msg_tokenied  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                                          \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                                       \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, then, say]                                                                                                                              \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]                                                                                                            \n",
       "\n",
       "                                                                                                                                                  msg_no_stopwords  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                                               \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                    \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3  [u, dun, say, early, hor, u, c, already, say]                                                                                                                     \n",
       "4  [nah, dont, think, goes, usf, lives, around, though]                                                                                                              \n",
       "\n",
       "                                                                                                                                                    msg_lemmatized  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                                               \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                    \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3  [u, dun, say, early, hor, u, c, already, say]                                                                                                                     \n",
       "4  [nah, dont, think, go, usf, life, around, though]                                                                                                                 \n",
       "\n",
       "                                                                                                                                                    msg_stemmed  \n",
       "0  [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]                                                                 \n",
       "1  [ok, lar, joke, wif, u, oni]                                                                                                                                  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]  \n",
       "3  [u, dun, say, earli, hor, u, c, alreadi, say]                                                                                                                 \n",
       "4  [nah, dont, think, goe, usf, live, around, though]                                                                                                            "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download necessary NLTK data\n",
    "\n",
    "def perform_stemming(tokens):\n",
    "    # Initialize the SnowballStemmer with the desired language\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    # Perform stemming on each token\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# Apply perform_stemming function to 'msg_no_stopwords' column\n",
    "df['msg_stemmed'] = df['msg_no_stopwords'].apply(perform_stemming)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try it out with porter and lancaster stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570a7a4",
   "metadata": {},
   "source": [
    "#### Lemmatiization\n",
    "It stems the word but makes sure that it does not lose its meaning.  Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary while diminishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc9798b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>msg_lower</th>\n",
       "      <th>msg_tokenied</th>\n",
       "      <th>msg_no_stopwords</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "      <th>msg_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0  ham    \n",
       "1  ham    \n",
       "2  spam   \n",
       "3  ham    \n",
       "4  ham    \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                               \n",
       "1  Ok lar... Joking wif u oni...                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3  U dun say so early hor... U c already then say...                                                                                                             \n",
       "4  Nah I don't think he goes to usf, he lives around here though                                                                                                 \n",
       "\n",
       "                                                                                                                                               clean_msg  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                                                  \n",
       "1  Ok lar Joking wif u oni                                                                                                                                 \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3  U dun say so early hor U c already then say                                                                                                             \n",
       "4  Nah I dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                               msg_lower  \\\n",
       "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat                                                  \n",
       "1  ok lar joking wif u oni                                                                                                                                 \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3  u dun say so early hor u c already then say                                                                                                             \n",
       "4  nah i dont think he goes to usf he lives around here though                                                                                             \n",
       "\n",
       "                                                                                                                                                                         msg_tokenied  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                                          \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                                       \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, then, say]                                                                                                                              \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]                                                                                                            \n",
       "\n",
       "                                                                                                                                                  msg_no_stopwords  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                                               \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                    \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3  [u, dun, say, early, hor, u, c, already, say]                                                                                                                     \n",
       "4  [nah, dont, think, goes, usf, lives, around, though]                                                                                                              \n",
       "\n",
       "                                                                                                                                                    msg_lemmatized  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                                               \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                    \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3  [u, dun, say, early, hor, u, c, already, say]                                                                                                                     \n",
       "4  [nah, dont, think, go, usf, life, around, though]                                                                                                                 \n",
       "\n",
       "                                                                                                                                                    msg_stemmed  \n",
       "0  [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]                                                                 \n",
       "1  [ok, lar, joke, wif, u, oni]                                                                                                                                  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]  \n",
       "3  [u, dun, say, earli, hor, u, c, alreadi, say]                                                                                                                 \n",
       "4  [nah, dont, think, goe, usf, live, around, though]                                                                                                            "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "\n",
    "df['msg_lemmatized']=df['msg_no_stopwords'].apply(lambda x:lemmatizer(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fee546",
   "metadata": {},
   "source": [
    "in the last row we can see goes has changed to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64bae0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classification  \\\n",
       "0  ham             \n",
       "1  ham             \n",
       "2  spam            \n",
       "3  ham             \n",
       "4  ham             \n",
       "\n",
       "                                                                                                                                                           Context  \n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                                              \n",
       "1  [ok, lar, joking, wif, u, oni]                                                                                                                                   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "3  [u, dun, say, early, hor, u, c, already, say]                                                                                                                    \n",
       "4  [nah, dont, think, go, usf, life, around, though]                                                                                                                "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame with only the first and last columns\n",
    "df1 = pd.DataFrame({'Classification': df.iloc[:, 0], 'Context': df.iloc[:, -2]})\n",
    "\n",
    "# Display the new DataFrame\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c257e",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "CountVectorizer is a feature extraction technique used in natural language processing (NLP) to convert a collection of text documents into a matrix of token counts. It is a part of the scikit-learn library in Python.\n",
    "\n",
    "Here's how CountVectorizer works:\n",
    "\n",
    "- Tokenization: It first tokenizes the text, splitting it into individual words or terms. It can optionally preprocess the text by converting it to lowercase, removing accents, or applying custom tokenization rules.\n",
    "\n",
    "- Counting: It then counts the frequency of each term in each document. The result is a matrix where each row represents a document, each column represents a unique term (word), and each cell contains the count of how many times the term appears in the corresponding document.\n",
    "\n",
    "- Vectorization: The resulting matrix is often sparse because most documents only contain a subset of the entire vocabulary. It converts this sparse matrix into a dense matrix suitable for input into machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53b4c7",
   "metadata": {},
   "source": [
    "###### Parameters\n",
    "\n",
    "-max_features: Specifies the maximum number of features (terms) to consider. Only the most frequent terms are kept if specified.\n",
    "\n",
    "-stop_words: Specifies a list of words to be ignored during tokenization, typically common words like \"the\", \"and\", etc.\n",
    "\n",
    "-ngram_range: Specifies the range of n-grams to consider. An n-gram is a contiguous sequence of n items from a given sample of text (e.g., words, characters).\n",
    "\n",
    "-binary: If True, the presence or absence of a term is considered, rather than its frequenc meaning each term in the document is represented as either 1 (if the term is present in the document) or 0 (if the term is absent).\n",
    "\n",
    "**Try tuning the vectorizer with the above parameters!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17f63907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   008704050406  0089my  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0  0             0       0     0            0            0           0    \n",
       "1  0             0       0     0            0            0           0    \n",
       "2  0             0       0     0            0            0           0    \n",
       "3  0             0       0     0            0            0           0    \n",
       "4  0             0       0     0            0            0           0    \n",
       "\n",
       "   020603  0207  02070836089  ...  ìï  ìïll  ûthanks  ûªm  ûªt  ûªve  ûï  \\\n",
       "0  0       0     0            ...  0   0     0        0    0    0     0    \n",
       "1  0       0     0            ...  0   0     0        0    0    0     0    \n",
       "2  0       0     0            ...  0   0     0        0    0    0     0    \n",
       "3  0       0     0            ...  0   0     0        0    0    0     0    \n",
       "4  0       0     0            ...  0   0     0        0    0    0     0    \n",
       "\n",
       "   ûïharry  ûò  ûówell  \n",
       "0  0        0   0       \n",
       "1  0        0   0       \n",
       "2  0        0   0       \n",
       "3  0        0   0       \n",
       "4  0        0   0       \n",
       "\n",
       "[5 rows x 8829 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert the lists of tokens into strings\n",
    "df1['Context'] = df1['Context'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Convert the text tokens into numerical features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df1['Context'])\n",
    "\n",
    "# Convert the sparse matrix to a DataFrame\n",
    "X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame containing the count of each word\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd049608",
   "metadata": {},
   "source": [
    "**Joining the 'Classification' column back to the DataFrame containing the word counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bdc79c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classification  008704050406  0089my  0121  01223585236  01223585334  \\\n",
       "0  ham            0             0       0     0            0             \n",
       "1  ham            0             0       0     0            0             \n",
       "2  spam           0             0       0     0            0             \n",
       "3  ham            0             0       0     0            0             \n",
       "4  ham            0             0       0     0            0             \n",
       "\n",
       "   0125698789  02  020603  0207  ...  ìï  ìïll  ûthanks  ûªm  ûªt  ûªve  ûï  \\\n",
       "0  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "1  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "2  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "3  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "4  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "\n",
       "   ûïharry  ûò  ûówell  \n",
       "0  0        0   0       \n",
       "1  0        0   0       \n",
       "2  0        0   0       \n",
       "3  0        0   0       \n",
       "4  0        0   0       \n",
       "\n",
       "[5 rows x 8830 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the Classification column with the DataFrame containing word counts\n",
    "df3 = pd.concat([df1['Classification'], X_df], axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b771fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 8830)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c200162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classification  008704050406  0089my  0121  01223585236  01223585334  \\\n",
       "0  1               0             0       0     0            0             \n",
       "1  1               0             0       0     0            0             \n",
       "2  0               0             0       0     0            0             \n",
       "3  1               0             0       0     0            0             \n",
       "4  1               0             0       0     0            0             \n",
       "\n",
       "   0125698789  02  020603  0207  ...  ìï  ìïll  ûthanks  ûªm  ûªt  ûªve  ûï  \\\n",
       "0  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "1  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "2  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "3  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "4  0           0   0       0     ...  0   0     0        0    0    0     0    \n",
       "\n",
       "   ûïharry  ûò  ûówell  \n",
       "0  0        0   0       \n",
       "1  0        0   0       \n",
       "2  0        0   0       \n",
       "3  0        0   0       \n",
       "4  0        0   0       \n",
       "\n",
       "[5 rows x 8830 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'ham' with 1 and 'spam' with 0 in the 'Classification' column\n",
    "df3['Classification'] = df3['Classification'].replace({'ham': 1, 'spam': 0})\n",
    "\n",
    "# Display the DataFrame after replacing values\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5771acd",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model\n",
    "\n",
    "Logistic Regression is used for predicting the probability of a binary outcome (yes/no, 1/0, true/false) based on one or more predictor variables. Despite its name, logistic regression is actually a classification algorithm, not a regression algorithm.\n",
    "\n",
    "- It squeezes the range of output values to exist only between 0 and 1\n",
    "- it has a point of inflection which can be used to separate the feature space into two distinct areas\n",
    "- It is only effective in cases we have linearity seperable data\n",
    "\n",
    "**Linearly separable\"** refers to the property of a dataset where the classes (categories or labels) can be perfectly separated by a linear decision boundary. In other words, if you can draw a straight line (or hyperplane in higher dimensions) that completely separates the instances of one class from the instances of another class, then the dataset is said to be linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be99614",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification model. It compares the actual labels of a dataset with the labels predicted by the model. The confusion matrix is especially useful for understanding the types of errors that a model is making and provides insights into its strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5eb6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91       150\n",
      "           1       0.98      1.00      0.99       965\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Coefficients: [[-0.01764915 -0.02244956 -0.28785433 ... -0.13109832  0.10314806\n",
      "   0.00880045]]\n",
      "Intercept: [4.23625106]\n",
      "Predicted Probabilities:\n",
      "[[0.0378138  0.9621862 ]\n",
      " [0.0353678  0.9646322 ]\n",
      " [0.47548274 0.52451726]\n",
      " ...\n",
      " [0.00390266 0.99609734]\n",
      " [0.00796229 0.99203771]\n",
      " [0.17390999 0.82609001]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df3.drop(columns=['Classification'])\n",
    "y = df3['Classification']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients and intercept\n",
    "coefficients = logreg_model.coef_\n",
    "intercept = logreg_model.intercept_\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display the coefficients and intercept\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "# Get predicted probabilities for each class\n",
    "y_proba = logreg_model.predict_proba(X_test)\n",
    "print(\"Predicted Probabilities:\")\n",
    "print(y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751c440",
   "metadata": {},
   "source": [
    "**Precision:** The precision measures the proportion of correctly predicted instances among all instances predicted as positive. For class 0 (spam), it is 1.00, indicating that all instances predicted as spam are indeed spam. For class 1 (ham), it is 0.98, indicating that 98% of instances predicted as ham are actually ham.\n",
    "\n",
    "**Recall:** The recall measures the proportion of correctly predicted instances of a class among all instances of that class in the dataset. For class 0 (spam), it is 0.84, indicating that 84% of actual spam instances are correctly classified as spam. For class 1 (ham), it is 1.00, indicating that all actual ham instances are correctly classified as ham.\n",
    "\n",
    "**F1-score:** The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. For class 0 (spam), it is 0.91, and for class 1 (ham), it is 0.99.\n",
    "\n",
    "**Accuracy:** The accuracy measures the proportion of correctly predicted instances among all instances in the dataset. It is 0.98, indicating that 98% of instances are correctly classified overall.\n",
    "\n",
    "**Support:** The support is the number of actual occurrences of each class in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd7d091",
   "metadata": {},
   "source": [
    "**Intercept:** The intercept is the value of the decision boundary where the predicted probability of the positive class (ham) equals 0.5. In logistic regression, it represents the log-odds of the positive class when all predictor variables are zero. In this case, the intercept is approximately 4.24.\n",
    "\n",
    "**Coefficients:** Coefficients represent the change in the log-odds of the positive class (ham) for a one-unit change in the corresponding predictor variable, holding all other variables constant. Each coefficient corresponds to a feature (word in this case) and indicates the strength and direction of its influence on the prediction. Positive coefficients indicate a positive association with the positive class (ham), while negative coefficients indicate a negative association. In this example, the coefficients represent the log-odds changes for each word in predicting ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b37679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>126</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>0</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0  126          24         \n",
       "Actual 1  0            965        "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test and y_pred are the actual and predicted labels, respectively\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Convert confusion matrix to DataFrame\n",
    "confusion_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88efbf",
   "metadata": {},
   "source": [
    "Predicted 0 corresponds to the model's predictions for the negative class (spam).\n",
    "\n",
    "Predicted 1 corresponds to the model's predictions for the positive class (ham).\n",
    "\n",
    "Actual 0 represents the actual instances of the negative class (spam).\n",
    "\n",
    "Actual 1 represents the actual instances of the positive class (ham).\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "True Negatives (TN): The model correctly predicted 126 instances as spam (negative class) when they were actually spam.\n",
    "\n",
    "False Positives (FP): The model incorrectly predicted 24 instances as ham (positive class) when they were actually spam.\n",
    "\n",
    "True Positives (TP): The model correctly predicted 965 instances as ham when they were actually ham.\n",
    "\n",
    "False Negatives (FN): The model did not incorrectly predict any instances as spam when they were actually ham."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
